# ============================================================
# API Keys (REQUIRED)
# ============================================================
# Service: https://aoai-farm.bosch-temp.com
# This key provides access to both embedding generation and LLM queries
# Note: 6M token/month limit
LLM_FARM_API_KEY=your_api_key_here

# ============================================================
# LLM Configuration
# ============================================================
# Model name for LLM responses
# Default: gemini-2.0-flash-lite
# Options: gemini-2.0-flash-lite, gemini-2.0-flash, etc.
LLM_MODEL=gemini-2.0-flash-lite

# Temperature controls randomness (0.0 = deterministic, 2.0 = very random)
# Default: 0.7
# Range: 0.0 to 2.0
LLM_TEMPERATURE=0.7

# Maximum tokens in LLM response
# Default: 2048
# Range: 1 to 4096 (model dependent)
LLM_MAX_TOKENS=2048

# ============================================================
# Embedding Configuration
# ============================================================
# Model name for text embeddings
# Default: askbosch-prod-farm-openai-text-embedding-3-small
EMBEDDING_MODEL=askbosch-prod-farm-openai-text-embedding-3-small

# ============================================================
# Prompt Template
# ============================================================
# Option 1: Use built-in template name
# Options: default, concise, detailed, technical
# Default: default
PROMPT_TEMPLATE=default

# Option 2: Use custom template file
# If set, this overrides PROMPT_TEMPLATE
# Path can be absolute or relative to project root
# PROMPT_TEMPLATE_PATH=./prompts/custom.prompt.txt

# ============================================================
# Chunking Configuration
# ============================================================
# Number of characters per chunk
# Default: 500
# Recommendation: 300-1000 depending on document type
CHUNK_SIZE=500

# Number of overlapping characters between chunks
# Default: 50
# Recommendation: 10-20% of CHUNK_SIZE for better context
CHUNK_OVERLAP=50

# ============================================================
# Search Configuration
# ============================================================
# Number of most similar chunks to retrieve for context
# Default: 3
# Range: 1-10 (higher = more context, but may introduce noise)
TOP_K=3

# ============================================================
# Performance & Reliability
# ============================================================
# Number of chunks to process before saving (checkpoint)
# Default: 50
# Higher = faster but more loss if interrupted
CHECKPOINT_INTERVAL=50

# Maximum number of retries for failed API calls
# Default: 3
MAX_RETRIES=3

# Delay in milliseconds between retries
# Default: 1000 (1 second)
# Uses exponential backoff: attempt * RETRY_DELAY_MS
RETRY_DELAY_MS=1000

# Timeout for embedding API calls in milliseconds
# Default: 30000 (30 seconds)
EMBEDDING_API_TIMEOUT_MS=30000

# Timeout for LLM API calls in milliseconds
# Default: 60000 (60 seconds)
LLM_API_TIMEOUT_MS=60000

# ============================================================
# Proxy Configuration (for Bosch network)
# ============================================================
# Enable HTTP proxy for API calls
# Default: false
# Set to true if behind corporate firewall
# PROXY_ENABLED=true

# Proxy host
# Default: 127.0.0.1
# PROXY_HOST=127.0.0.1

# Proxy port
# Default: 3128
# PROXY_PORT=3128

# ============================================================
# Paths
# ============================================================
# Directory containing PDF documents to index
# Default: ./documents
DOCUMENTS_PATH=./documents

# Directory for storing collection embeddings
# Default: ./data/collections
COLLECTIONS_PATH=./data/collections

# Directory for storing text chunks
# Default: ./data/chunks
CHUNKS_PATH=./data/chunks

# Directory containing prompt templates
# Default: ./prompts
PROMPTS_PATH=./prompts
